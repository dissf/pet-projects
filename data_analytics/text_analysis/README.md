# Text_analysis

## Оглавление

[1. Основные цели и задачи проекта](https://github.com/dissf/pet-projects/blob/main/DA_DS/text_analysis/README.md#Основные-цели-и-задачи-проекта)  
[2. Краткая информация о данных](https://github.com/dissf/pet-projects/blob/main/DA_DS/text_analysis/README.md#Краткая-информация-о-данных)  
[3. Полезные библиотеки](https://github.com/dissf/pet-projects/blob/main/DA_DS/text_analysis/README.md#Полезные-библиотеки)

### Основные цели и задачи проекта

Спарсить информацию о всех видео с канала и всех соответствующей им статистикой, в том числе субтитры, выяснить частоту появления нецензурной лексики в каждом видео.

### Краткая информация о данных

Датасет отсутствует, необходимо его создать.
Данные парсятся с сайта youtube.
Получаем два датасета: **df** и **df_text**.

Описание полей датасета **df**:

* **title** - название видео<br>
* **id** - id видео<br>
* **published_at** - время и дата публикации видео<br>
* **description** - описание видео<br>
* **viewCount** - количество просмотров<br>
* **likeCount** - количество лайков<br>
* **commentCount** - количество комментариев<br>
* **duration** - длительность видео в секундах

Описание полей датасета **df_text**:

* **title** - название видео<br>
* **text** - текст видео из субтитров<br>
* **dirt_min** - количество нецензурных слов в минуту<br>
* **dirt** - общее количество нецензурных слов<br>
* **duration** - длительность видео в секундах<br>
* **total** - всего слов в видео<br>
* **uniq** - число уникальных слов в видео<br>
* **percent** - отношение уникальных слов ко всем

### Полезные библиотеки

* pandas  
* re  
* pymorphy2
* nltk
* youtube_transcript_api
* plotly
